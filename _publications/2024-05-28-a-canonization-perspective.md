---
title: "A Canonization Perspective on Invariant and Equivariant Learning"
collection: publications
permalink: /publication/2024-05-28-a-canonization-perspective
excerpt: 'We analysed the efficiency and expressiveness of invariant and equivariant networks from a canonization perspective.'
date: 2023-09-22
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2405.18378'
citation: 'George Ma, Yifei Wang, Derek Lim, Stefanie Jegelka, Yisen Wang (2024). A Canonization Perspective on Invariant and Equivariant Learning. <i>arXiv preprint arXiv:2405.18378</i>.'
---
In many applications, we desire neural networks to exhibit invariance or equivariance to certain groups due to symmetries inherent in the data. Recently, frame-averaging methods emerged to be a unified framework for attaining symmetries efficiently by averaging over input-dependent subsets of the group, i.e., frames. What we currently lack is a principled understanding of the design of frames. In this work, we introduce a canonization perspective that provides an essential and complete view of the design of frames. We show that there exists an inherent connection between frames and canonical forms. Leveraging this connection, we can efficiently compare the complexity of frames as well as determine the optimality of certain frames. We solve an open problem regarding the expressiveness of invariant networks with equivariance constraints, using canonization theory. We also design novel frames for eigenvectors that are strictly superior to existing methods -- some are even optimal -- both theoretically and empirically. The reduction to the canonization perspective further uncovers equivalences between previous methods. These observations suggest that canonization provides a fundamental understanding of existing frame-averaging methods and unifies existing equivariant and invariant learning methods.